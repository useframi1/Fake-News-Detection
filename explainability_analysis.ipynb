{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.data_preparation_pipeline import DataPreparationPipeline\n",
    "from fake_news_classifier import FakeNewsClassifier\n",
    "from utils.utils import load_config, set_device\n",
    "import shap\n",
    "import lime\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from data.testing_dataset import TestingDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation_pipeline = DataPreparationPipeline(\n",
    "    \"configs/pipelines_config/data_preparation_config.json\"\n",
    ")\n",
    "train_data, test_data, val_data = data_preparation_pipeline.run()\n",
    "\n",
    "fake_news_classifier = FakeNewsClassifier(\"configs/classifier_config.json\", 7)\n",
    "model = fake_news_classifier.load_pretrained(\"models/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"configs/pipelines_config/explainability_config.json\")\n",
    "device = set_device()\n",
    "model = model.classifier\n",
    "val_data = val_data\n",
    "num_background = config[\"num_background\"]\n",
    "num_explain_samples = config[\"num_explain_samples\"]\n",
    "\n",
    "contents = val_data[\"content\"].to_list()\n",
    "labels = val_data[\"label\"].to_list()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(texts):\n",
    "    test_dataset = TestingDataset(texts, tokenizer)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    probabilities = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, token_type_ids = (\n",
    "                batch[\"input_ids\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "                batch[\"token_type_ids\"].to(device),\n",
    "            )\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            probabilities.extend(\n",
    "                torch.nn.functional.softmax(outputs, dim=1).cpu().tolist()\n",
    "            )\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef50a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shap():\n",
    "    print(\"Starting SHAP explainability analysis...\")\n",
    "\n",
    "    label_map = {\n",
    "        0: \"reliable\",\n",
    "        1: \"bias\",\n",
    "        2: \"conspiracy\",\n",
    "        3: \"fake\",\n",
    "        4: \"rumor\",\n",
    "        5: \"unreliable\",\n",
    "        6: \"other\",\n",
    "    }\n",
    "\n",
    "    class_contents = {i: [] for i in range(len(label_map))}\n",
    "    for content, label in zip(contents, labels):\n",
    "        class_contents[label].append(content)\n",
    "\n",
    "    sampled_contents = []\n",
    "    for class_label, class_data in class_contents.items():\n",
    "        sampled_contents.extend(\n",
    "            np.random.choice(class_data, size=min(10, len(class_data)), replace=False)\n",
    "        )\n",
    "\n",
    "    num_explain_samples = len(sampled_contents)\n",
    "\n",
    "    masker = shap.maskers.Text(tokenizer)\n",
    "\n",
    "    output_names = [label_map[i] for i in range(len(label_map))]\n",
    "    explainer = shap.Explainer(predict_proba, masker, output_names=output_names)\n",
    "    shap_values = explainer(sampled_contents)\n",
    "\n",
    "    print(\"SHAP values computed successfully.\")\n",
    "\n",
    "    print(shap.plots.text(shap_values))\n",
    "\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "# def run_shap():\n",
    "#     print(\"Starting SHAP explainability analysis...\")\n",
    "\n",
    "#     # Assuming 'contents' and 'labels' (the class labels) are defined\n",
    "#     # 'contents' should be the text data, and 'labels' should be the corresponding class labels\n",
    "#     label_map = {\n",
    "#         0: \"reliable\",\n",
    "#         1: \"bias\",\n",
    "#         2: \"conspiracy\",\n",
    "#         3: \"fake\",\n",
    "#         4: \"rumor\",\n",
    "#         5: \"unreliable\",\n",
    "#         6: \"other\",\n",
    "#     }\n",
    "\n",
    "#     # Group the contents by class\n",
    "#     class_contents = {i: [] for i in range(len(label_map))}\n",
    "#     for content, label in zip(contents, labels):\n",
    "#         class_contents[label].append(content)\n",
    "\n",
    "#     # Sample 10 contents per class\n",
    "#     sampled_contents = []\n",
    "#     for class_label, class_data in class_contents.items():\n",
    "#         # Sample 10 items per class, or all if fewer than 10\n",
    "#         sampled_contents.extend(\n",
    "#             np.random.choice(class_data, size=min(10, len(class_data)), replace=False)\n",
    "#         )\n",
    "\n",
    "#     # The number of explain samples will be the total number of sampled contents\n",
    "#     num_explain_samples = len(sampled_contents)\n",
    "\n",
    "#     # Create a masker for text\n",
    "#     masker = shap.maskers.Text(tokenizer)\n",
    "\n",
    "#     # Initialize SHAP Explainer\n",
    "#     output_names = [label_map[i] for i in range(len(label_map))]\n",
    "#     explainer = shap.Explainer(predict_proba, masker, output_names=output_names)\n",
    "#     shap_values = explainer(sampled_contents)\n",
    "\n",
    "#     print(\"SHAP values computed successfully.\")\n",
    "\n",
    "#     # Calculate the absolute SHAP values for each token\n",
    "#     shap_values_array = np.array([np.abs(sv.values).max() for sv in shap_values])\n",
    "\n",
    "#     # Calculate the threshold based on the 90th percentile (you can change this to 95, etc.)\n",
    "#     threshold = np.percentile(shap_values_array, 90)\n",
    "#     print(f\"Threshold (90th percentile): {threshold}\")\n",
    "\n",
    "#     # Create filtered shap_values based on the threshold\n",
    "#     filtered_shap_values = shap.Explanation(\n",
    "#         values=[sv.values for sv in shap_values if np.abs(sv.values).max() > threshold],\n",
    "#         base_values=[\n",
    "#             sv.base_values for sv in shap_values if np.abs(sv.values).max() > threshold\n",
    "#         ],\n",
    "#         data=[sv.data for sv in shap_values if np.abs(sv.values).max() > threshold],\n",
    "#         feature_names=shap_values[0].feature_names,\n",
    "#         output_names=shap_values[0].output_names,\n",
    "#     )\n",
    "\n",
    "#     # Plot the filtered SHAP values using the text plot\n",
    "#     print(shap.plots.text(filtered_shap_values))\n",
    "\n",
    "#     return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398865ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = run_shap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
